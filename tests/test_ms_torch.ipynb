{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSNorm 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops\n",
    "\n",
    "def torch_rms_layernorm(hidden: mindspore.Tensor, weight: mindspore.Tensor, eps: float):\n",
    "    old_dtype = hidden.dtype\n",
    "    variance = hidden.to(mindspore.float32).pow(2).mean(axis=-1, keep_dims=True)\n",
    "    hidden = (hidden * ops.rsqrt(variance + eps)).to(old_dtype)\n",
    "    return hidden * weight\n",
    "\n",
    "\n",
    "class MSMiniCPMRMSNorm(nn.Cell):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MiniCPMRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = mindspore.Parameter(ops.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def construct(self, hidden_states):\n",
    "        return ms_rms_layernorm(hidden_states, self.weight, self.variance_epsilon)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.78732544 1.2833312  1.2922165  ... 0.04089143 0.49289078 0.6821702 ]\n",
      "  [1.1579348  0.84069234 0.76891506 ... 1.5036066  1.2710027  0.71515465]\n",
      "  [1.1578155  0.05839312 0.49893308 ... 0.69267124 0.08189241 0.7144707 ]\n",
      "  ...\n",
      "  [0.29475603 0.5684639  0.6187994  ... 1.5154487  1.4241893  0.34242994]\n",
      "  [0.51642406 0.667304   1.2583175  ... 1.3388052  1.2167487  0.94261634]\n",
      "  [1.1134353  1.1357505  1.1854455  ... 1.2128674  1.0710244  0.49770108]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hidden_size = 4096\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "\n",
    "ms_nms_norm = MSMiniCPMRMSNorm(hidden_size)\n",
    "\n",
    "\n",
    "input_array = np.random.rand(batch_size, seq_len, hidden_size)\n",
    "input_array = input_array.astype(np.float32)\n",
    "ms_input = mindspore.Tensor.from_numpy(input_array)\n",
    "\n",
    "\n",
    "ms_output = ms_nms_norm(ms_input)\n",
    "print(ms_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def torch_rms_layernorm(hidden: torch.Tensor, weight: torch.Tensor, eps: float):\n",
    "    old_dtype = hidden.dtype\n",
    "    variance = hidden.to(torch.float32).pow(2).mean(dim=-1, keepdim=True)\n",
    "    hidden = (hidden * torch.rsqrt(variance + eps)).to(old_dtype)\n",
    "    return hidden * weight\n",
    "\n",
    "\n",
    "class TorchMiniCPMRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MiniCPMRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return torch_rms_layernorm(hidden_states, self.weight, self.variance_epsilon)\n",
    "    \n",
    "torch_nms_norm = TorchMiniCPMRMSNorm(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7873, 1.2833, 1.2922,  ..., 0.0409, 0.4929, 0.6822],\n",
      "         [1.1579, 0.8407, 0.7689,  ..., 1.5036, 1.2710, 0.7152],\n",
      "         [1.1578, 0.0584, 0.4989,  ..., 0.6927, 0.0819, 0.7145],\n",
      "         ...,\n",
      "         [0.2948, 0.5685, 0.6188,  ..., 1.5154, 1.4242, 0.3424],\n",
      "         [0.5164, 0.6673, 1.2583,  ..., 1.3388, 1.2167, 0.9426],\n",
      "         [1.1134, 1.1358, 1.1854,  ..., 1.2129, 1.0710, 0.4977]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.from_numpy(input_array)\n",
    "torch_output = torch_nms_norm(torch_input)\n",
    "print(torch_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output.asnumpy(), torch_output.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RotrayEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops\n",
    "\n",
    "class MSMiniCPMRotaryEmbedding(nn.Cell):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        self.inv_freq = 1.0 / (self.base ** (ops.arange(0, self.dim, 2).to(mindspore.float32) / self.dim))\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            # seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "            seq_len=max_position_embeddings, dtype=mindspore.float32\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = ops.arange(end=self.max_seq_len_cached, dtype=self.inv_freq.dtype)\n",
    "        freqs = ops.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = ops.cat((freqs, freqs), axis=-1)\n",
    "\n",
    "        self.cos_cached = emb.cos().to(dtype)\n",
    "        self.sin_cached = emb.sin().to(dtype)\n",
    "\n",
    "\n",
    "    def construct(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[10, 4096], dtype=Float32, value=\n",
      "[[ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00 ...  1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
      " [ 5.40302277e-01,  5.44072688e-01,  5.47815144e-01 ...  1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
      " [-4.16146845e-01, -4.07969773e-01, -3.99797082e-01 ...  1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
      " ...\n",
      " [ 7.53902256e-01,  7.74163365e-01,  7.93574035e-01 ...  9.99999762e-01,  9.99999762e-01,  9.99999762e-01],\n",
      " [-1.45500034e-01, -1.09898202e-01, -7.43169263e-02 ...  9.99999642e-01,  9.99999702e-01,  9.99999702e-01],\n",
      " [-9.11130250e-01, -8.93748343e-01, -8.74997675e-01 ...  9.99999583e-01,  9.99999583e-01,  9.99999583e-01]]), Tensor(shape=[10, 4096], dtype=Float32, value=\n",
      "[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
      " [ 8.41470957e-01,  8.39038074e-01,  8.36599410e-01 ...  1.01358310e-04,  1.00903497e-04,  1.00450736e-04],\n",
      " [ 9.09297407e-01,  9.12995458e-01,  9.16603684e-01 ...  2.02716619e-04,  2.01806994e-04,  2.00901472e-04],\n",
      " ...\n",
      " [ 6.56986594e-01,  6.32985830e-01,  6.08473718e-01 ...  7.09508138e-04,  7.06324412e-04,  7.03155121e-04],\n",
      " [ 9.89358246e-01,  9.93942857e-01,  9.97234702e-01 ...  8.10866361e-04,  8.07227858e-04,  8.03605828e-04],\n",
      " [ 4.12118495e-01,  4.48568672e-01,  4.84127164e-01 ...  9.12224641e-04,  9.08131362e-04,  9.04056476e-04]]))\n"
     ]
    }
   ],
   "source": [
    "num_attn_heads = 32\n",
    "head_size = hidden_size // num_attn_heads\n",
    "\n",
    "ms_rotary_emb = MSMiniCPMRotaryEmbedding(dim=hidden_size)\n",
    "\n",
    "input_array_rope = np.random.rand(batch_size, num_attn_heads, seq_len, head_size).astype(np.float32)\n",
    "ms_input_rope = mindspore.Tensor.from_numpy(input_array_rope)\n",
    "\n",
    "ms_output_rope = ms_rotary_emb(ms_input_rope, seq_len=seq_len)\n",
    "print(ms_output_rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TorchMiniCPMRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            # seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.5441,  0.5478,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4161, -0.4080, -0.3998,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        ...,\n",
      "        [ 0.7539,  0.7742,  0.7936,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.1455, -0.1099, -0.0743,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.9111, -0.8937, -0.8750,  ...,  1.0000,  1.0000,  1.0000]]), tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [8.4147e-01, 8.3904e-01, 8.3660e-01,  ..., 1.0136e-04, 1.0090e-04,\n",
      "         1.0045e-04],\n",
      "        [9.0930e-01, 9.1300e-01, 9.1660e-01,  ..., 2.0272e-04, 2.0181e-04,\n",
      "         2.0090e-04],\n",
      "        ...,\n",
      "        [6.5699e-01, 6.3299e-01, 6.0847e-01,  ..., 7.0951e-04, 7.0632e-04,\n",
      "         7.0316e-04],\n",
      "        [9.8936e-01, 9.9394e-01, 9.9723e-01,  ..., 8.1087e-04, 8.0723e-04,\n",
      "         8.0361e-04],\n",
      "        [4.1212e-01, 4.4857e-01, 4.8413e-01,  ..., 9.1222e-04, 9.0813e-04,\n",
      "         9.0406e-04]]))\n"
     ]
    }
   ],
   "source": [
    "torch_rotary_emb = TorchMiniCPMRotaryEmbedding(dim=hidden_size, device=torch.device(\"cpu\"))\n",
    "\n",
    "input_array_rope = np.random.rand(batch_size, seq_len, hidden_size).astype(np.float32)\n",
    "torch_input_rope = torch.from_numpy(input_array_rope)\n",
    "\n",
    "torch_output_rope = torch_rotary_emb(torch_input_rope, seq_len=seq_len)\n",
    "print(torch_output_rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "ms_output_rope_cos, ms_output_rope_sin = ms_output_rope\n",
    "torch_output_rope_cos, torch_output_rope_sin = torch_output_rope\n",
    "print(np.allclose(ms_output_rope_cos.asnumpy(), torch_output_rope_cos.detach().numpy(), atol=1e-3))\n",
    "print(np.allclose(ms_output_rope_sin.asnumpy(), torch_output_rope_sin.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearScalingRoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMiniCPMLinearScalingRotaryEmbedding(MSMiniCPMRotaryEmbedding):\n",
    "    \"\"\"MiniCPMRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = ops.arange(end=self.max_seq_len_cached, dtype=self.inv_freq.dtype)\n",
    "        t = t / self.scaling_factor\n",
    "\n",
    "        freqs = ops.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = ops.cat((freqs, freqs), axis=-1)\n",
    "        self.cos_cached = emb.cos().to(dtype)\n",
    "        self.sin_cached = emb.sin().to(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_rotary_emb_linear_scaling = MSMiniCPMLinearScalingRotaryEmbedding(dim=hidden_size)\n",
    "ms_rotary_emb_linear_scaling._set_cos_sin_cache(seq_len, mindspore.float32)\n",
    "ms_output_cos = ms_rotary_emb_linear_scaling.cos_cached\n",
    "ms_output_sin = ms_rotary_emb_linear_scaling.sin_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchMiniCPMLinearScalingRotaryEmbedding(TorchMiniCPMRotaryEmbedding):\n",
    "    \"\"\"MiniCPMRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base, device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        t = t / self.scaling_factor\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_rotary_emb_linear_scaling = TorchMiniCPMLinearScalingRotaryEmbedding(dim=hidden_size, device=torch.device(\"cpu\"))\n",
    "torch_rotary_emb_linear_scaling._set_cos_sin_cache(seq_len, torch.device(\"cpu\"), torch.float32)\n",
    "torch_output_cos = torch_rotary_emb_linear_scaling.cos_cached\n",
    "torch_output_sin = torch_rotary_emb_linear_scaling.sin_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output_sin.asnumpy(), torch_output_sin.detach().numpy(), atol=1e-3))\n",
    "print(np.allclose(ms_output_cos.asnumpy(), torch_output_cos.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamicNTK Scaling RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMiniCPMDynamicNTKScalingRotaryEmbedding(MSMiniCPMRotaryEmbedding):\n",
    "    \"\"\"MiniCPMRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "\n",
    "        if seq_len > self.max_position_embeddings:\n",
    "            base = self.base * (\n",
    "                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n",
    "            ) ** (self.dim / (self.dim - 2))\n",
    "            self.inv_freq = 1.0 / (base ** (ops.arange(0, self.dim, 2).to(mindspore.float32) / self.dim))\n",
    "\n",
    "        t = ops.arange(end=self.max_seq_len_cached, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = ops.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = ops.cat((freqs, freqs), axis=-1)\n",
    "\n",
    "        self.cos_cached = emb.cos().to(dtype)\n",
    "        self.sin_cached = emb.sin().to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4096)\n"
     ]
    }
   ],
   "source": [
    "ms_rotary_emb_dynamic_scaling = MSMiniCPMDynamicNTKScalingRotaryEmbedding(hidden_size)\n",
    "ms_rotary_emb_dynamic_scaling._set_cos_sin_cache(seq_len, mindspore.float32)\n",
    "ms_output_cos = ms_rotary_emb_dynamic_scaling.cos_cached\n",
    "ms_output_sin = ms_rotary_emb_dynamic_scaling.sin_cached\n",
    "\n",
    "print(ms_output_cos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchMiniCPMDynamicNTKScalingRotaryEmbedding(TorchMiniCPMRotaryEmbedding):\n",
    "    \"\"\"MiniCPMRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base, device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "\n",
    "        if seq_len > self.max_position_embeddings:\n",
    "            base = self.base * (\n",
    "                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n",
    "            ) ** (self.dim / (self.dim - 2))\n",
    "            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_rotary_emb_dynamic_scaling = TorchMiniCPMDynamicNTKScalingRotaryEmbedding(dim=hidden_size, device=torch.device(\"cpu\"))\n",
    "torch_rotary_emb_dynamic_scaling._set_cos_sin_cache(seq_len, torch.device(\"cpu\"), torch.float32)\n",
    "torch_output_cos = torch_rotary_emb_dynamic_scaling.cos_cached\n",
    "torch_output_sin = torch_rotary_emb_dynamic_scaling.sin_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output_sin.asnumpy(), torch_output_sin.detach().numpy(), atol=1e-3))\n",
    "print(np.allclose(ms_output_cos.asnumpy(), torch_output_cos.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rotate half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import ops\n",
    "\n",
    "def ms_rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return ops.cat((-x2, x1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.9383789  -0.7959062  -0.7229492  ...  0.75974023  0.57705337\n",
      "     0.5258102 ]\n",
      "   [-0.4408688  -0.0808837  -0.4772482  ...  0.64175427  0.0088954\n",
      "     0.8439258 ]\n",
      "   [-0.10451046 -0.24723798 -0.3941851  ...  0.4869339   0.5427277\n",
      "     0.27998877]\n",
      "   ...\n",
      "   [-0.9992323  -0.6110815  -0.51049364 ...  0.48206657  0.1275518\n",
      "     0.65963066]\n",
      "   [-0.81961334 -0.60193527 -0.16736974 ...  0.6794498   0.08161078\n",
      "     0.70404893]\n",
      "   [-0.9148321  -0.9682962  -0.4853782  ...  0.50420827  0.48732084\n",
      "     0.74103487]]\n",
      "\n",
      "  [[-0.9561082  -0.4691347  -0.20137118 ...  0.7968146   0.76639074\n",
      "     0.5343106 ]\n",
      "   [-0.8239253  -0.8691968  -0.26496986 ...  0.8805403   0.87118614\n",
      "     0.86909795]\n",
      "   [-0.90201867 -0.92350847 -0.4557022  ...  0.7991929   0.14225402\n",
      "     0.89251596]\n",
      "   ...\n",
      "   [-0.5357112  -0.12910734 -0.87065876 ...  0.967807    0.48311362\n",
      "     0.9123667 ]\n",
      "   [-0.18272442 -0.80684733 -0.6087966  ...  0.69655305  0.7977314\n",
      "     0.33861685]\n",
      "   [-0.24963343 -0.2640282  -0.60354143 ...  0.02268196  0.8105394\n",
      "     0.9327752 ]]\n",
      "\n",
      "  [[-0.8062207  -0.85483426 -0.96537876 ...  0.56921965  0.79547334\n",
      "     0.558357  ]\n",
      "   [-0.91422445 -0.54081315 -0.49233812 ...  0.36484003  0.9543222\n",
      "     0.76630235]\n",
      "   [-0.44866765 -0.09583676 -0.412877   ...  0.6039182   0.51060206\n",
      "     0.9326394 ]\n",
      "   ...\n",
      "   [-0.7381373  -0.4257624  -0.9028472  ...  0.04227767  0.09073028\n",
      "     0.08712819]\n",
      "   [-0.5302964  -0.00329506 -0.02506017 ...  0.6041799   0.5405524\n",
      "     0.6521353 ]\n",
      "   [-0.75277174 -0.94844604 -0.07308502 ...  0.5262139   0.5902586\n",
      "     0.4510747 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.12369382 -0.51582813 -0.75985175 ...  0.4603967   0.03890992\n",
      "     0.8121475 ]\n",
      "   [-0.598555   -0.3139033  -0.5975563  ...  0.3601818   0.40116057\n",
      "     0.5169496 ]\n",
      "   [-0.92757183 -0.27416953 -0.7619469  ...  0.7859173   0.27249724\n",
      "     0.5441009 ]\n",
      "   ...\n",
      "   [-0.15333709 -0.540994   -0.5121195  ...  0.59852153  0.7159219\n",
      "     0.10828026]\n",
      "   [-0.09581242 -0.98299754 -0.73728645 ...  0.19957459  0.01784792\n",
      "     0.06151072]\n",
      "   [-0.85075474 -0.09753907 -0.51523066 ...  0.17134923  0.13314599\n",
      "     0.27924776]]\n",
      "\n",
      "  [[-0.47375095 -0.266955   -0.64517474 ...  0.9842661   0.14856863\n",
      "     0.28567487]\n",
      "   [-0.69692224 -0.7815152  -0.9909688  ...  0.98228186  0.15369274\n",
      "     0.36679897]\n",
      "   [-0.8164548  -0.48316434 -0.2566842  ...  0.59779775  0.02503975\n",
      "     0.38758963]\n",
      "   ...\n",
      "   [-0.24494801 -0.15484004 -0.7764563  ...  0.9559565   0.05262414\n",
      "     0.84839845]\n",
      "   [-0.15809439 -0.5896423  -0.39698333 ...  0.49273005  0.44043744\n",
      "     0.53022814]\n",
      "   [-0.44726086 -0.69478804 -0.9490663  ...  0.54525125  0.65061045\n",
      "     0.15576738]]\n",
      "\n",
      "  [[-0.03282161 -0.42514253 -0.43967083 ...  0.4754555   0.44515824\n",
      "     0.33525425]\n",
      "   [-0.6161997  -0.86731714 -0.11474282 ...  0.5453677   0.32278955\n",
      "     0.50943977]\n",
      "   [-0.39991045 -0.5293621  -0.7567265  ...  0.58077425  0.5548292\n",
      "     0.04655554]\n",
      "   ...\n",
      "   [-0.20606135 -0.1176784  -0.34747383 ...  0.3842549   0.02486336\n",
      "     0.23227224]\n",
      "   [-0.15325077 -0.4834892  -0.22679225 ...  0.8292127   0.6452473\n",
      "     0.06695828]\n",
      "   [-0.70355535 -0.22241566 -0.27520654 ...  0.71521163  0.6013397\n",
      "     0.21952212]]]]\n"
     ]
    }
   ],
   "source": [
    "input_array = np.random.rand(batch_size, num_attn_heads, seq_len, head_size).astype(np.float32)\n",
    "\n",
    "ms_input = mindspore.Tensor.from_numpy(input_array)\n",
    "ms_output = ms_rotate_half(ms_input)\n",
    "\n",
    "print(ms_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def torch_rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.9384, -0.7959, -0.7229,  ...,  0.7597,  0.5771,  0.5258],\n",
      "          [-0.4409, -0.0809, -0.4772,  ...,  0.6418,  0.0089,  0.8439],\n",
      "          [-0.1045, -0.2472, -0.3942,  ...,  0.4869,  0.5427,  0.2800],\n",
      "          ...,\n",
      "          [-0.9992, -0.6111, -0.5105,  ...,  0.4821,  0.1276,  0.6596],\n",
      "          [-0.8196, -0.6019, -0.1674,  ...,  0.6794,  0.0816,  0.7040],\n",
      "          [-0.9148, -0.9683, -0.4854,  ...,  0.5042,  0.4873,  0.7410]],\n",
      "\n",
      "         [[-0.9561, -0.4691, -0.2014,  ...,  0.7968,  0.7664,  0.5343],\n",
      "          [-0.8239, -0.8692, -0.2650,  ...,  0.8805,  0.8712,  0.8691],\n",
      "          [-0.9020, -0.9235, -0.4557,  ...,  0.7992,  0.1423,  0.8925],\n",
      "          ...,\n",
      "          [-0.5357, -0.1291, -0.8707,  ...,  0.9678,  0.4831,  0.9124],\n",
      "          [-0.1827, -0.8068, -0.6088,  ...,  0.6966,  0.7977,  0.3386],\n",
      "          [-0.2496, -0.2640, -0.6035,  ...,  0.0227,  0.8105,  0.9328]],\n",
      "\n",
      "         [[-0.8062, -0.8548, -0.9654,  ...,  0.5692,  0.7955,  0.5584],\n",
      "          [-0.9142, -0.5408, -0.4923,  ...,  0.3648,  0.9543,  0.7663],\n",
      "          [-0.4487, -0.0958, -0.4129,  ...,  0.6039,  0.5106,  0.9326],\n",
      "          ...,\n",
      "          [-0.7381, -0.4258, -0.9028,  ...,  0.0423,  0.0907,  0.0871],\n",
      "          [-0.5303, -0.0033, -0.0251,  ...,  0.6042,  0.5406,  0.6521],\n",
      "          [-0.7528, -0.9484, -0.0731,  ...,  0.5262,  0.5903,  0.4511]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1237, -0.5158, -0.7599,  ...,  0.4604,  0.0389,  0.8121],\n",
      "          [-0.5986, -0.3139, -0.5976,  ...,  0.3602,  0.4012,  0.5169],\n",
      "          [-0.9276, -0.2742, -0.7619,  ...,  0.7859,  0.2725,  0.5441],\n",
      "          ...,\n",
      "          [-0.1533, -0.5410, -0.5121,  ...,  0.5985,  0.7159,  0.1083],\n",
      "          [-0.0958, -0.9830, -0.7373,  ...,  0.1996,  0.0178,  0.0615],\n",
      "          [-0.8508, -0.0975, -0.5152,  ...,  0.1713,  0.1331,  0.2792]],\n",
      "\n",
      "         [[-0.4738, -0.2670, -0.6452,  ...,  0.9843,  0.1486,  0.2857],\n",
      "          [-0.6969, -0.7815, -0.9910,  ...,  0.9823,  0.1537,  0.3668],\n",
      "          [-0.8165, -0.4832, -0.2567,  ...,  0.5978,  0.0250,  0.3876],\n",
      "          ...,\n",
      "          [-0.2449, -0.1548, -0.7765,  ...,  0.9560,  0.0526,  0.8484],\n",
      "          [-0.1581, -0.5896, -0.3970,  ...,  0.4927,  0.4404,  0.5302],\n",
      "          [-0.4473, -0.6948, -0.9491,  ...,  0.5453,  0.6506,  0.1558]],\n",
      "\n",
      "         [[-0.0328, -0.4251, -0.4397,  ...,  0.4755,  0.4452,  0.3353],\n",
      "          [-0.6162, -0.8673, -0.1147,  ...,  0.5454,  0.3228,  0.5094],\n",
      "          [-0.3999, -0.5294, -0.7567,  ...,  0.5808,  0.5548,  0.0466],\n",
      "          ...,\n",
      "          [-0.2061, -0.1177, -0.3475,  ...,  0.3843,  0.0249,  0.2323],\n",
      "          [-0.1533, -0.4835, -0.2268,  ...,  0.8292,  0.6452,  0.0670],\n",
      "          [-0.7036, -0.2224, -0.2752,  ...,  0.7152,  0.6013,  0.2195]]]])\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.from_numpy(input_array)\n",
    "torch_output = torch_rotate_half(torch_input)\n",
    "\n",
    "print(torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output.asnumpy(), torch_output.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`mindspore.Tensor`): The query tensor.\n",
    "        k (`mindspore.Tensor`): The key tensor.\n",
    "        cos (`mindspore.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`mindspore.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`mindspore.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(mindspore.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    # cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    # sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    # q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    # k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    orig_dtype = k.dtype\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)  # [bs, 1, seq_len, dim]\n",
    "    q_fp32 = q.to(dtype=mindspore.float32)\n",
    "    k_fp32 = k.to(dtype=mindspore.float32)\n",
    "    q_embed = (q_fp32 * cos) + (ms_rotate_half(q_fp32) * sin)\n",
    "    k_embed = (k_fp32 * cos) + (ms_rotate_half(k_fp32) * sin)\n",
    "    return q_embed.to(dtype=orig_dtype), k_embed.to(dtype=orig_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_qk_array = np.random.rand(batch_size, seq_len, hidden_size).astype(np.float32)\n",
    "ms_q = ms_k = mindspore.Tensor.from_numpy(input_qk_array)\n",
    "ms_cos = ms_sin = mindspore.ops.ones((batch_size, seq_len, hidden_size), dtype=mindspore.float32)\n",
    "position_ids = ops.arange(end=batch_size)\n",
    "\n",
    "ms_output_k, ms_output_q = ms_apply_rotary_pos_emb(ms_q, ms_k, ms_cos, ms_sin, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    # cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    # sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    # q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    # k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    orig_dtype = k.dtype\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)  # [bs, 1, seq_len, dim]\n",
    "    q_fp32 = q.to(dtype=torch.float32, device=q.device)\n",
    "    k_fp32 = k.to(dtype=torch.float32, device=k.device)\n",
    "    q_embed = (q_fp32 * cos) + (torch_rotate_half(q_fp32) * sin)\n",
    "    k_embed = (k_fp32 * cos) + (torch_rotate_half(k_fp32) * sin)\n",
    "    return q_embed.to(dtype=orig_dtype), k_embed.to(dtype=orig_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_q = torch_k = torch.from_numpy(input_qk_array)\n",
    "torch_cos = torch_sin = torch.ones((batch_size, seq_len, hidden_size)).float()\n",
    "position_ids = torch.arange(end=batch_size)\n",
    "\n",
    "torch_output_k, torch_output_q = torch_apply_rotary_pos_emb(torch_q, torch_k, torch_cos, torch_sin, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output_k.asnumpy(), torch_output_k.detach().numpy(), atol=1e-3))\n",
    "print(np.allclose(ms_output_q.asnumpy(), torch_output_q.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPMMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops\n",
    "\n",
    "class MSMiniCPMMLP(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Dense(self.hidden_size, self.intermediate_size, has_bias=False)\n",
    "        self.up_proj = nn.Dense(self.hidden_size, self.intermediate_size, has_bias=False)\n",
    "        self.down_proj = nn.Dense(self.intermediate_size, self.hidden_size, has_bias=False)\n",
    "        self.act_fn = config.hidden_act\n",
    "\n",
    "    def construct(self, x):\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            slice = self.intermediate_size // self.config.pretraining_tp\n",
    "            gate_proj_slices = self.gate_proj.weight.split(slice, axis=0)\n",
    "            up_proj_slices = self.up_proj.weight.split(slice, axis=0)\n",
    "            down_proj_slices = self.down_proj.weight.split(slice, axis=1)\n",
    "\n",
    "            gate_proj = ops.cat(\n",
    "                [ops.dense(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], axis=-1\n",
    "            )\n",
    "            up_proj = ops.cat([ops.dense(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], axis=-1)\n",
    "\n",
    "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, axis=2)\n",
    "            down_proj = [\n",
    "                ops.dense(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            down_proj = sum(down_proj)\n",
    "        else:\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCPMConfig:\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`MiniCPMModel`]. It is used to instantiate an MiniCPM\n",
    "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
    "    defaults will yield a similar configuration to that of the MiniCPM-7B.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 32000):\n",
    "            Vocabulary size of the MiniCPM model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`MiniCPMModel`]\n",
    "        hidden_size (`int`, *optional*, defaults to 4096):\n",
    "            Dimension of the hidden representations.\n",
    "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
    "            Dimension of the MLP representations.\n",
    "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
    "            Number of hidden layers in the Transformer decoder.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder.\n",
    "        num_key_value_heads (`int`, *optional*):\n",
    "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
    "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
    "            by meanpooling all the original heads within that group. For more details checkout [this\n",
    "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
    "            `num_attention_heads`.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
    "            The non-linear activation function (function or string) in the decoder.\n",
    "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
    "            The maximum sequence length that this model might ever be used with. MiniCPM 1 supports up to 2048 tokens,\n",
    "            MiniCPM 2 up to 4096, CodeMiniCPM up to 16384.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
    "            The epsilon used by the rms normalization layers.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`.\n",
    "        pad_token_id (`int`, *optional*):\n",
    "            Padding token id.\n",
    "        bos_token_id (`int`, *optional*, defaults to 1):\n",
    "            Beginning of stream token id.\n",
    "        eos_token_id (`int`, *optional*, defaults to 2):\n",
    "            End of stream token id.\n",
    "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n",
    "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to tie weight embeddings\n",
    "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "            The base period of the RoPE embeddings.\n",
    "        rope_scaling (`Dict`, *optional*):\n",
    "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
    "            these scaling strategies behave:\n",
    "            https://www.reddit.com/r/LocalMiniCPM/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
    "            experimental feature, subject to breaking API changes in future versions.\n",
    "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
    "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import MiniCPMModel, MiniCPMConfig\n",
    "\n",
    "    >>> # Initializing a MiniCPM minicpm-7b style configuration\n",
    "    >>> configuration = MiniCPMConfig()\n",
    "\n",
    "    >>> # Initializing a model from the minicpm-7b style configuration\n",
    "    >>> model = MiniCPMModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"minicpm\"\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32000,\n",
    "        hidden_size=4096,\n",
    "        intermediate_size=11008,\n",
    "        num_hidden_layers=32,\n",
    "        num_attention_heads=32,\n",
    "        num_key_value_heads=None,\n",
    "        hidden_act=nn.SiLU(),\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        use_cache=True,\n",
    "        pad_token_id=None,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        pretraining_tp=1,\n",
    "        tie_word_embeddings=True,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=0.0,\n",
    "        scale_emb=1,\n",
    "        dim_model_base=1,\n",
    "        scale_depth=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # for backward compatibility\n",
    "        if num_key_value_heads is None:\n",
    "            num_key_value_heads = num_attention_heads\n",
    "\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.pretraining_tp = pretraining_tp\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        # self._rope_scaling_validation()\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.scale_emb = scale_emb\n",
    "        self.dim_model_base = dim_model_base\n",
    "        self.scale_depth = scale_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_config = MiniCPMConfig()\n",
    "ms_mlp = MSMiniCPMMLP(ms_config)\n",
    "\n",
    "input_array = np.random.rand(batch_size, 2048, hidden_size).astype(np.float32)\n",
    "\n",
    "ms_input_mlp = mindspore.Tensor.from_numpy(input_array)\n",
    "ms_output_mlp = ms_mlp(ms_input_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "class TorchMiniCPMMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            slice = self.intermediate_size // self.config.pretraining_tp\n",
    "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
    "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
    "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
    "\n",
    "            gate_proj = torch.cat(\n",
    "                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n",
    "            )\n",
    "            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n",
    "\n",
    "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
    "            down_proj = [\n",
    "                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            down_proj = sum(down_proj)\n",
    "        else:\n",
    "            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n",
    "# and OPT implementations in this library. It has been modified from its\n",
    "# original forms to accommodate minor architectural differences compared\n",
    "# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" MiniCPM model configuration\"\"\"\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "MINICPM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n",
    "\n",
    "\n",
    "class MiniCPMConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`MiniCPMModel`]. It is used to instantiate an MiniCPM\n",
    "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
    "    defaults will yield a similar configuration to that of the MiniCPM-7B.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 32000):\n",
    "            Vocabulary size of the MiniCPM model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`MiniCPMModel`]\n",
    "        hidden_size (`int`, *optional*, defaults to 4096):\n",
    "            Dimension of the hidden representations.\n",
    "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
    "            Dimension of the MLP representations.\n",
    "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
    "            Number of hidden layers in the Transformer decoder.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder.\n",
    "        num_key_value_heads (`int`, *optional*):\n",
    "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
    "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
    "            by meanpooling all the original heads within that group. For more details checkout [this\n",
    "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
    "            `num_attention_heads`.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
    "            The non-linear activation function (function or string) in the decoder.\n",
    "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
    "            The maximum sequence length that this model might ever be used with. MiniCPM 1 supports up to 2048 tokens,\n",
    "            MiniCPM 2 up to 4096, CodeMiniCPM up to 16384.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
    "            The epsilon used by the rms normalization layers.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`.\n",
    "        pad_token_id (`int`, *optional*):\n",
    "            Padding token id.\n",
    "        bos_token_id (`int`, *optional*, defaults to 1):\n",
    "            Beginning of stream token id.\n",
    "        eos_token_id (`int`, *optional*, defaults to 2):\n",
    "            End of stream token id.\n",
    "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n",
    "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to tie weight embeddings\n",
    "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "            The base period of the RoPE embeddings.\n",
    "        rope_scaling (`Dict`, *optional*):\n",
    "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
    "            these scaling strategies behave:\n",
    "            https://www.reddit.com/r/LocalMiniCPM/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
    "            experimental feature, subject to breaking API changes in future versions.\n",
    "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
    "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import MiniCPMModel, MiniCPMConfig\n",
    "\n",
    "    >>> # Initializing a MiniCPM minicpm-7b style configuration\n",
    "    >>> configuration = MiniCPMConfig()\n",
    "\n",
    "    >>> # Initializing a model from the minicpm-7b style configuration\n",
    "    >>> model = MiniCPMModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"minicpm\"\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32000,\n",
    "        hidden_size=4096,\n",
    "        intermediate_size=11008,\n",
    "        num_hidden_layers=32,\n",
    "        num_attention_heads=32,\n",
    "        num_key_value_heads=None,\n",
    "        hidden_act=\"silu\",\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        use_cache=True,\n",
    "        pad_token_id=None,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        pretraining_tp=1,\n",
    "        tie_word_embeddings=True,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=0.0,\n",
    "        scale_emb=1,\n",
    "        dim_model_base=1,\n",
    "        scale_depth=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # for backward compatibility\n",
    "        if num_key_value_heads is None:\n",
    "            num_key_value_heads = num_attention_heads\n",
    "\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.pretraining_tp = pretraining_tp\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self._rope_scaling_validation()\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.scale_emb = scale_emb\n",
    "        self.dim_model_base = dim_model_base\n",
    "        self.scale_depth = scale_depth\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            tie_word_embeddings=tie_word_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        try:\n",
    "            import flash_attn\n",
    "            self._attn_implementation = \"flash_attention_2\"\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def _rope_scaling_validation(self):\n",
    "        \"\"\"\n",
    "        Validate the `rope_scaling` configuration.\n",
    "        \"\"\"\n",
    "        if self.rope_scaling is None:\n",
    "            return\n",
    "\n",
    "        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n",
    "            raise ValueError(\n",
    "                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, \"\n",
    "                f\"got {self.rope_scaling}\"\n",
    "            )\n",
    "        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n",
    "        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n",
    "        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n",
    "            raise ValueError(\n",
    "                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n",
    "            )\n",
    "        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n",
    "            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_config = MiniCPMConfig()\n",
    "torch_mlp = TorchMiniCPMMLP(torch_config)\n",
    "torch_input_mlp = torch.from_numpy(input_array)\n",
    "\n",
    "torch_output_mlp = torch_mlp(torch_input_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output_mlp.asnumpy(), torch_output_mlp.detach().numpy(), atol=1e-3))\n",
    "print(np.allclose(ms_input_mlp.asnumpy(), torch_input_mlp.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00555526  0.03499052  0.00351002 ... -0.03566707 -0.00674556\n",
      "   -0.00390526]\n",
      "  [ 0.00502021  0.04264032  0.04439148 ... -0.02678786  0.00661847\n",
      "   -0.03573214]\n",
      "  [ 0.00616076  0.01205319  0.04827144 ...  0.00361077 -0.01063555\n",
      "   -0.01574899]\n",
      "  ...\n",
      "  [-0.02905967 -0.02110439  0.02138827 ... -0.00567385  0.00530072\n",
      "   -0.03021811]\n",
      "  [-0.02721993  0.00997222 -0.00347654 ... -0.00987197 -0.00353347\n",
      "   -0.02058237]\n",
      "  [ 0.00977214  0.02368069 -0.00850708 ... -0.05252867  0.00523038\n",
      "   -0.03006126]]]\n"
     ]
    }
   ],
   "source": [
    "print(ms_output_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0011,  0.0185,  0.0542,  ...,  0.0349,  0.0352, -0.0069],\n",
       "         [-0.0247,  0.0302, -0.0036,  ...,  0.0202,  0.0188, -0.0153],\n",
       "         [-0.0372, -0.0314, -0.0384,  ...,  0.0185,  0.0270, -0.0268],\n",
       "         ...,\n",
       "         [-0.0097, -0.0449, -0.0001,  ..., -0.0157,  0.0637, -0.0172],\n",
       "         [ 0.0173,  0.0053, -0.0149,  ...,  0.0137,  0.0653, -0.0643],\n",
       "         [-0.0006,  0.0005, -0.0208,  ...,  0.0389,  0.0164, -0.0161]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.7589559  -0.07527499 -0.35819474 ... -0.49136406  0.22672285\n",
      "    0.03544176]\n",
      "  [ 0.4955714  -0.2900765  -0.5850546  ... -0.6383949   0.276924\n",
      "   -0.37251896]\n",
      "  [ 0.8016432  -0.04432888 -0.61939305 ... -0.50569415  0.34891298\n",
      "   -0.03819778]\n",
      "  ...\n",
      "  [ 0.76070076 -0.33570215 -0.5777197  ... -0.46006587  0.5309063\n",
      "   -0.5314831 ]\n",
      "  [ 0.9872962  -0.05854413 -0.63073397 ... -0.59783506  0.34394875\n",
      "   -0.35045716]\n",
      "  [ 0.73312736 -0.2669691  -0.4728214  ... -0.30531365  0.5313557\n",
      "   -0.46166003]]]\n",
      "[[[ 0.11332035  0.474159   -0.01539239 ...  0.10245368  0.05044889\n",
      "   -0.250153  ]\n",
      "  [ 0.10101779  0.01157233 -0.38048276 ...  0.11030661  0.25223908\n",
      "   -0.32689023]\n",
      "  [-0.02514513  0.1507451  -0.37613693 ...  0.39953572 -0.04060515\n",
      "   -0.40098608]\n",
      "  ...\n",
      "  [-0.18186352  0.0609047  -0.32666564 ... -0.05875109  0.44832134\n",
      "   -0.10809004]\n",
      "  [ 0.0401681  -0.1633428  -0.5373877  ...  0.05558265  0.23133233\n",
      "   -0.50210017]\n",
      "  [-0.24035767  0.30383867 -0.5683611  ... -0.02691719  0.10517808\n",
      "   -0.3336593 ]]]\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "input_array = np.load(\"./input_array.npy\").astype(np.float32)\n",
    "\n",
    "torch_net = nn.Linear(4096, 11008, bias=False)\n",
    "x = torch.from_numpy(input_array)\n",
    "torch_output = torch_net(x)\n",
    "print(torch_output.detach().numpy())\n",
    "# (2, 4)\n",
    "\n",
    "# MindSpore\n",
    "import mindspore\n",
    "from mindspore import Tensor, nn\n",
    "import numpy as np\n",
    "\n",
    "x = mindspore.Tensor.from_numpy(input_array)\n",
    "ms_net = nn.Dense(4096, 11008, has_bias=False)\n",
    "ms_output = ms_net(x)\n",
    "print(ms_output)\n",
    "# (2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00408853 -0.0072077  -0.01317369 ... -0.01395462 -0.0130742\n",
      "  -0.01503178]\n",
      " [-0.01060252 -0.00302445 -0.00668621 ...  0.00487923  0.01343558\n",
      "   0.00919498]\n",
      " [ 0.00601849 -0.01478642 -0.01136461 ... -0.0005769  -0.00788664\n",
      "   0.01203256]\n",
      " ...\n",
      " [ 0.00654013  0.00516639 -0.00797721 ...  0.00618667  0.00966447\n",
      "  -0.01506723]\n",
      " [-0.00121664 -0.00063749 -0.00711064 ... -0.01112679 -0.01172142\n",
      "   0.0137059 ]\n",
      " [-0.00770224  0.00940624 -0.01486338 ... -0.00967028 -0.006234\n",
      "  -0.00682555]]\n"
     ]
    }
   ],
   "source": [
    "print(ms_net.weight.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0004, -0.0023,  0.0156,  ...,  0.0138, -0.0082,  0.0101],\n",
      "        [ 0.0138, -0.0060,  0.0135,  ..., -0.0085,  0.0012,  0.0061],\n",
      "        [-0.0013,  0.0037,  0.0005,  ...,  0.0149,  0.0034,  0.0112],\n",
      "        ...,\n",
      "        [-0.0037, -0.0051, -0.0134,  ..., -0.0050, -0.0096,  0.0031],\n",
      "        [-0.0104, -0.0088,  0.0093,  ...,  0.0087, -0.0028, -0.0149],\n",
      "        [-0.0148, -0.0138, -0.0059,  ..., -0.0099, -0.0034, -0.0148]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(torch_net.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048, 11008)\n",
      "torch.Size([1, 2048, 11008])\n"
     ]
    }
   ],
   "source": [
    "print(ms_output.shape)\n",
    "print(torch_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0102,  0.0066,  0.0002,  ..., -0.0097, -0.0008,  0.0030],\n",
      "        [ 0.0154,  0.0099, -0.0091,  ...,  0.0092, -0.0035,  0.0094],\n",
      "        [ 0.0108, -0.0143, -0.0117,  ..., -0.0146, -0.0131, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0138,  0.0054, -0.0034,  ...,  0.0134,  0.0105, -0.0056],\n",
      "        [ 0.0100, -0.0059,  0.0095,  ...,  0.0115, -0.0059, -0.0018],\n",
      "        [ 0.0024,  0.0020, -0.0050,  ..., -0.0013, -0.0100,  0.0147]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "torch_net2 = nn.Linear(4096, 11008, bias=False)\n",
    "print(torch_net2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops\n",
    "import numpy as np\n",
    "\n",
    "hidden_size = 4096\n",
    "num_attn_heads = 32\n",
    "seq_len = 10\n",
    "batch_size = 1\n",
    "n_rep = 3\n",
    "head_size = hidden_size // num_attn_heads\n",
    "\n",
    "input_array_repeat_kv = np.random.rand(batch_size, num_attn_heads, seq_len, head_size).astype(np.float32)\n",
    "ms_input_repeat_kv = mindspore.Tensor(input_array_repeat_kv)\n",
    "\n",
    "def ms_repeat_kv(hidden_states: mindspore.Tensor, n_rep: int) -> mindspore.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of ops.repeat_interleave(input, repeats=n_rep, axis=1). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].broadcast_to((batch, num_key_value_heads, n_rep, slen, head_dim))\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "ms_output_repeat_kv = ms_repeat_kv(ms_input_repeat_kv, n_rep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def torch_repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "torch_input_repeat_kv = torch.from_numpy(input_array_repeat_kv)\n",
    "torch_output_repeat_kv = torch_repeat_kv(torch_input_repeat_kv, n_rep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(ms_output_repeat_kv.asnumpy(), torch_output_repeat_kv.detach().numpy(), atol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "\n",
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(28*28, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 10, weight_init=\"normal\", bias_init=\"zeros\")\n",
    "        )\n",
    "        self.class_name = self.__class__.__name__\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        print(self.class_name)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore_2.2.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
